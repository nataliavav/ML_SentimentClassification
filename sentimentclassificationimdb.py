# -*- coding: utf-8 -*-
"""SentimentClassificationIMDB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H3ZRxulY3o30x-DXQUpaR2T9kw-HU4lv

# Python Sentiment Classification for database of imdb reviews

-Preprocessing dataframe 

-Using tfidf to vectorize the review text into trainable data

-Training our classifier with training data using logistic regression

-Making predictions on validation data

-Attempting to achieve best f1,precision and recall scores in our validation data by changing the hyperparameters of logistic regression
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/gdrive')

"""Read and print imdb database file from drive"""

df = pd.read_csv("/content/gdrive/MyDrive/imdb-reviews.csv", sep='\t')
print(df)

"""Preprocessing - Data Cleansing"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop = stopwords.words('english')

def preprocess(df):
  df = df.drop('url', axis = 1)
  df['review'] = df['review'].str.lower()
  df['review'] = df['review'].str.replace("<br />", " ")
  df['review'] = df['review'].str.replace(r'http\S+', '', regex=True).replace(r'www\S+', '', regex=True)    # Remove urls
  df['review'] = df['review'].str.replace('@[A-Za-z0-9_]+','')                                              # Remove mentions
  df = df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))                  # Remove emojis
  df['review'] = df['review'].str.replace('[^\w\s]','')                                                     # Remove punctuation
  df['review'] = df['review'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))   # Remove stop words
  return df

df1 = preprocess(df)
print(df1)

"""Print the 20 most used words in the database"""

most_used_words = pd.Series(' '.join(df1['review']).split()).value_counts()[:20]
print(most_used_words)

"""Preprocessing - Setting training and validation set."""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split


X = df1.drop('rating', axis=1)
X = X.squeeze()
Y = df1[['rating']]                 # only keep the rating
Y = Y.replace(['7.0', '8.0', '9.0', '10.0'], 1)
Y = Y.replace(['0.0', '1.0', '2.0', '3.0', '4.0'], 0)


X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.15, random_state=7)

"""Vectorize with Tfidf Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfvectorizer = TfidfVectorizer(min_df=0.01)
X_train_tfidf = tfvectorizer.fit_transform(X_train)

X_val_tfidf = tfvectorizer.transform(X_val)

print(X_train_tfidf.shape)
print(X_val_tfidf.shape)

"""Example using what I think are the best hyperparameters with tfidf vectorizer.
Includes f1, precision and recall scores
"""

# Training
classifier = LogisticRegression(C=0.1, solver='newton-cg', penalty='l2', max_iter=2000)
classifier.fit(X_train_tfidf, np.ravel(Y_train))

#Results
predictions_train = classifier.predict(X_train_tfidf)
predictions_val = classifier.predict(X_val_tfidf)

print("Predictions by model:")
print(predictions_val)
print("Actual reviews (Positive or Negative):")
print(np.ravel(Y_val))

# F-Score

f1_train = f1_score(Y_train, predictions_train)
f1_val = f1_score(Y_val, predictions_val)

print("\nTraining Set:")
print("F1 Score:", f1_train)

print("\nValidation Set:")
print("F1 Score:", f1_val)
pscore = precision_score(predictions_val, Y_val, average='macro')
print('Precision:', pscore)
rscore = recall_score(predictions_val, Y_val, average='macro')
print('Recall:', rscore)

"""Plot using best hyperparameters with tfidf vectorizer, to show that my model is neither over or underfitting"""

import matplotlib.pyplot as plt

list_f1=[]
list_f1_train=[]
list_sample_size=[]

for times in range(10):
  
  # Training
  X, X_unused, y, y_unused = train_test_split(X_train_tfidf, Y_train, test_size=1 - (times * 0.1 + 0.005), random_state=0)

  classifier = LogisticRegression(C=0.1, solver='newton-cg', penalty='l2', max_iter=2000)
  classifier.fit(X, np.ravel(y))

  results_train = classifier.predict(X)

  # Validation
  results = classifier.predict(X_val_tfidf)

  # Score
  f1_train = f1_score(y, results_train)
  print("F1 Score Train: " + str(f1_train))

  f1 = f1_score(Y_val, results)
  print("F1 Score Validation: " + str(f1))

  list_f1.append(f1)
  list_f1_train.append(f1_train)
  list_sample_size.append((times * 0.1 + 0.1))

plt.plot(list_sample_size, list_f1)
plt.plot(list_sample_size, list_f1_train)

plt.ylim(ymin=0)
plt.legend(["Validation", "Training"])